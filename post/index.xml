<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Programming Blog</title>
    <link>https://pritamps.github.io/post/</link>
    <description>Recent content in Posts on Programming Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://pritamps.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning Tutorials</title>
      <link>https://pritamps.github.io/dl-tutorial-series/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/dl-tutorial-series/</guid>
      <description>&amp;mdash;- THESE POSTS ARE BROKEN &amp;mdash; PLEASE EXCUSE WHILE I FIX THEM
Welcome to my Deep Learning Tutorial Series!
Here are the posts in order so you can find them all from the same landing page:
 [Introduction to Deep Learning][week-1]: A bird&amp;rsquo;s eye of what deep learning is why it&amp;rsquo;s become so popular these days Logistic Regression and Neural Networks: An exploration of the idea behind Neural Networks (actually, just one neuron for this part) using a Logistic Regression, a a popular Machine Learning Algorithm.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-08-11-week-1-intro-to-nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-08-11-week-1-intro-to-nn/</guid>
      <description>Table of Contents {:.no_toc} * Do not remove this line (it will not be displayed) {:toc}
Overview Technically, Deep Learning is the application of Neural Networks where more than one hidden layer of neurons is involved. In the common form that it has pervaded the media today, it also usually involves a mixture of neural networks with other algorithms specifically applied to large datasets in a variety of areas.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-08-12-week-2-logistic-regression-and-neural-networks-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-08-12-week-2-logistic-regression-and-neural-networks-1/</guid>
      <description>In this post, we will go over the basics of the functioning of a neural network. The idea will be to use Logistic Regression and Gradient Descent to illustrate the fundamentally important concepts of forward propagation and backpropagation. As an example, we might write some code for image recognition, which should give you an idea of just how powerful neural networks.
The post loosely follows (with some edits and additions by me) the lectures in Week 2 of the Deep Learning Specialisation on coursera.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-08-15-week-2-part-2-lr-gradient-descent-and-neural-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-08-15-week-2-part-2-lr-gradient-descent-and-neural-networks/</guid>
      <description>In the [previous post][week-2-part-1], I introduced the basic idea behind logistic regression and the notation for:
 One input: x \in \mathbb{R}^{n_x} , a feature vector extracted from whatever our data source is, and n_x  is the number of features One training label: y \in {0,1} The weight and threshold: (w \in \mathbb{R}^{n_x}, b \in \mathbb{R}) are the weight vector and the threshold respectively The output: \hat{y} = \sigma(w^Tx + b)  where \sigma  represents the sigmoid function, and \hat{y}  represents the probability that y  is 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-08-19-week-2-part-3-optimise/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-08-19-week-2-part-3-optimise/</guid>
      <description>Welcome to Part 3 of explaining logistic regression using neural networks! We gave a medium size picture of the whole thing in [Part 1][week-2-part-1] and then defined the optimization problem in [Part 2][week-2-part-2]. In this episode, we&amp;rsquo;ll first develop an algorithm to solve the problem by iterating through the examples, and then use the awesome power of vectorization to go through all examples at once. So, let&amp;rsquo;s get started, yeah?</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-08-24-week-3-part-1-shallownnrepresentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-08-24-week-3-part-1-shallownnrepresentation/</guid>
      <description>Before we get neck-deep into deep neural networks, let&amp;rsquo;s wade into shallow waters and use a two layer network (one hidden layer, one output layer) to explore properites of neural networks in general. Let&amp;rsquo;s see how the notation extends to multiple layers, and what it means for our matrix computations and math.
Here&amp;rsquo;s the mother-diagram for the rest of this post.
{:refdef: style=&amp;ldquo;text-align: center;&amp;ldquo;} Why the ugly diagram? I just bought one of those drawing tables, so I&amp;rsquo;m learning how to use it!</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2017-09-23-deeplearning-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2017-09-23-deeplearning-example/</guid>
      <description>Instead of covering the same ground over and over (networks, layers, nodes, you know it all now!), I&amp;rsquo;m going to jump right into getting the actual code out.
Full disclaimer: This code is based on the assignments from the Coursera deeplearning course. So the structure of the final code I&amp;rsquo;ll develop will be the same as what Andrew Ng and co. use. The structure isn&amp;rsquo;t really the most efficient, but I&amp;rsquo;m treating this code as only a means to understanding the concepts.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pritamps.github.io/post/2018-12-27-mobilenet_summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pritamps.github.io/post/2018-12-27-mobilenet_summary/</guid>
      <description>Here, I write my notes on the MobileNet paper:
 MobileNet bucks the trend of achieving higher accuracy through deeper and more complicated networks by focusing on making an efficient network architecture.  Only two hyperparameters Low latency Low hardware requirements - suitable for mobile vision applications Allows for developer to choose a model that matches the resource restrictions  To reduce computation in first few layers, &amp;ldquo;depthwise separable convolutions [TODO]&amp;rdquo; are used.</description>
    </item>
    
  </channel>
</rss>