<!DOCTYPE html>
<html lang="en-us">
    <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="format-detection" content="telephone=no" />

  <title>
     | Programming Blog
  </title>

  
	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
	<link rel="manifest" href="/manifest.json" />
  <meta name="theme-color" content="#ffffff" />

  
  <link
    rel="stylesheet"
    href="https://unpkg.com/modern-normalize@0.6.0/modern-normalize.css"
  />

  
  
	
  <link rel="stylesheet" href="https://pritamps.github.io/css/basic.min.8be53fffd231757375f4b7eb78dd0e1515cd45a761fefd573af23ada70f7041f.css" />
  <link rel="stylesheet" href="https://pritamps.github.io/css/main.min.1274937ae5d78d419eff68b08446f83e3882aae191d47cacd17e9f02b2e75fcf.css" />

  
	
		
	
</head>

    <body>
        <header id="header">
  <div class="header_container">
    <h1 class="sitetitle">
      <a href="https://pritamps.github.io" title="Programming Blog">Programming Blog</a>
    </h1>
    <nav class="navbar">
      <ul>
        <li><a href="https://pritamps.github.io">Home</a></li>
        
        <li class="hide-sm"><a href="https://pritamps.github.io/index.xml" type="application/rss+xml">RSS</a></li>
      </ul>
    </nav>
  </div>
</header>

        
<section id="main">
  <article class="post content">
    <h2 class="title"></h2>
    <div class="post_content">
      

<h2 id="table-of-contents">Table of Contents</h2>

<p>{:.no_toc}
* Do not remove this line (it will not be displayed)
{:toc}</p>

<h2 id="overview">Overview</h2>

<p>Technically, <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> is the application of Neural Networks where more than one hidden layer of neurons is involved. In the common form that it has pervaded the media today, it also usually involves a mixture of neural networks with other algorithms specifically applied to large datasets in a variety of areas.</p>

<p>Don&rsquo;t know what that means? You&rsquo;re in the right place! The idea of this post is to cover just enough ground (in a very shallow way, pun sort-of intended) so that you can say the above paragraph out loud and understand what it means. So the following questions that I&rsquo;m sure are burning your stomach will be answered:</p>

<ol>
<li>What is a neuron?</li>
<li>What is a neural network?</li>
<li>What is deep learning?</li>
<li>Why has it taken off in the last decade or so?</li>
</ol>

<p>This post corresponds roughly to Week 1 in the <a href="https://www.coursera.org/specializations/deep-learning">Coursera Deep Learning Specialisation</a>.</p>

<p>So, let&rsquo;s get started!</p>

<h2 id="what-is-a-neuron">What is a Neuron?</h2>

<p>In the not-Computer-Science world a neuron is an organic thing in your body that is the basic unit of the nervous system. Any information that travels to your brain &ndash; heartbreak for example &ndash; goes through neurons. The way I understand it (it is important to note here that I am not a biologist), a neuron turns &ldquo;on&rdquo; at some level of electrical voltage. A pulse is carried through the neurons to the brain when they are all &ldquo;on&rdquo;. That&rsquo;s how information &ndash; the feeling you get when you resolve a bug in your code, for example &ndash; gets to your brain.</p>

<p>In the Computer Science world, it&rsquo;s the same, really. A neuron takes an input, applies a mathemtical transformation to it, and then gives an output.</p>

<p>&ldquo;Wait, isn&rsquo;t that the same as a mathematical function?&rdquo;, one of you asks.</p>

<p>Nope, nope. A neuron actually does two things to achieve its transformation:</p>

<ol>
<li>It applies a dot product to the inputs with the weights (the weights are a property of the neuron) and adds a threshold. For example, if the input vectors are of type <script type="math/tex"> \vec{x} \in \mathbb{R}<em>{3 \times 1}  </script>, the weights will be of the form <script type="math/tex"> \vec{w} \in \mathbb{R}</em>{3 \times 1} </script>, and the output of this step will be <script type="math/tex"> \vec{x} . \vec{w} + b </script></li>
<li>Applies an activation function to the result of 1 above, i.e <script type="math/tex"> f(\vec{x}.\vec{w} + b) </script> where <script type="math/tex"> f </script> is a function chosen by us.</li>
</ol>

<p>Here is a carefully constructed illutration that shows the in-depth workings of a single neuron as it is commonly represented in Computer Science.</p>

<p><img src="{{ site.url }}/assets/dl_week1/nn_basic.jpg" alt="A not-so-helpful illustration of a neuron" />
<em>This illustration doesn&rsquo;t illustrate much other than my inexperience with drawing software</em></p>

<p>&ldquo;Okay fine. The function that the neuron applies can be any linear function, right? What is the class of activation function that you can use?&rdquo;, someone pipes in from the back.</p>

<p>That&rsquo;s actually a very good question, thank you!</p>

<p>When neural networks were first being designed and used, the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> function was pretty popular. The idea of a sigmoid function is to kinda-sorta simulate the behaviour of a switch (Y = 0 when X is negative and Y = 1 when X is positive), but with the property that is continuous and differentiable everywhere. So if the output of our linear function is positive, you get a value close to 1, and if it&rsquo;s negative, you get a value of 0. Can you see how that will be useful if you are doing a classification problem where you either have to say &ldquo;Yes&rdquo; (1) or &ldquo;No&rdquo; (0)?</p>

<p>These days however, the RELU (REctified Linear Unit) is much more popular (shown in the ugly figure below). Apparently, it works much better with optimization algorithms such as Gradient Descent ü§∑‚Äç, even though there is that obvious discontinuity in the derivative. Maybe we&rsquo;ll figure out how this works some day in the future, you and me.</p>

<p><img src="{{ site.url }}/assets/dl_week1/relu.jpg" alt="REctified Linear Unit" />
<em>They could have just called it the RLU instead of RELU. Why the silly acronym?</em></p>

<p>So, to summarise, neuron takes input X, applies the RELU function to it, and generates Y as the output, which can either zero or positive. All clear? Good!</p>

<h2 id="neural-networks">Neural Networks</h2>

<p>I&rsquo;m sure many of you saw this coming, but guess what neural networks are? They&rsquo;re networks of neurons!</p>

<p>Neurons can be stacked together in a variety of ways, some of which are mind-bogglingly complex, but thankfully we don&rsquo;t have to think about that yet. Right now, imagine them stacked in layers, each layer feeding into the one ahead of it. Here&rsquo;s a figure for ya if my words aren&rsquo;t that well chosen.</p>

<p><img src="{{ site.url }}/assets/dl_week1/nn_with_layers.jpg" alt="A feedforward neural network" />
<em>A feedforward neural network. Note that the figure is incomplete. Each neuron can link to ALL neurons in the next layer</em></p>

<p>So here&rsquo;s what happens when you have the input vector:</p>

<ol>
<li>Each neuron in each layer has a set of weights. X is fed into the first layer. Each individual neuron outputs the thresholded dot product, to which RELU is applied.</li>
<li>The output of all those neurons are fed to the next layer.</li>
<li>This is repeated till suddenly, you have your output value Y.</li>
</ol>

<p>There&rsquo;s some handwaving there, I admit, because this stuff is still simmering in the pot of understanding of my mind. I&rsquo;ll update this post with more clearly chosen words over the next few weeks.</p>

<p>I know at least some of you are looking at that figure and thinking: How do you know how many layers to use? What do the layers mean? How do we decide the individual functions? Wait, what&rsquo;s happening?</p>

<p>So, here&rsquo;s what <em>we</em> decide:</p>

<ol>
<li>The number of layers between the input and output</li>
<li>The number of neurons in each layer</li>
<li>The function that each neuron applies. By &ldquo;decide&rdquo; here, I mean that each neuron is an RELU. You don&rsquo;t get to choose that for the most part!</li>
</ol>

<p><mark>A cool thing</mark>: The algorithm figures out everything else! Everything else, of course, involves the weights on the individual neuron. The middle layers are sometimes called &ldquo;hidden&rdquo;, because all you care about are X and Y, and the algorithm figures out everything in between</p>

<p><mark>Another cool thing</mark>: Does that figure look like your brain? I know it doesn&rsquo;t look like mine. It took a genius to make this connection: Walter Pitts. He spent most of his time chasing this idea of modeling the brain based on neural networks and unfortunately is not alive today to see the results of his work. You can read an excellent article about him and his amazing and sad life <a href="http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic">here</a></p>

<h3 id="applications-and-types-of-neural-networks">Applications and Types of Neural Networks</h3>

<p>Neural networks are used everywhere these days: from product recommendations to user-click probabilities, from image recognition to self driving cars. There are different types of neural networks, each of which we will get to at different points during this tutorial series:</p>

<ol>
<li>Standard Neural Nets: Like the ones shown in that awesomely drawn figure above</li>
<li>Convolutional Neural Nets: Each layer becomes multi-dimensional. Not sure what that means? To be honest, neither am I. We&rsquo;ll figure it out in a future post. For now, know that CNNs are all the rage in image processing these days</li>
<li>Recurrent Neural Networks: wWere we make use of sequential patterns in the data, like in natural language. So this is used in speech recognition, language processing, and those kinds of things</li>
<li>Custom/Hybrid: Where you have different techniques, you can mix-and-match. Custom NNs are used in complex applications such as self driving cars.</li>
</ol>

<p><mark>Important thing</mark>: These days, all the publicity with deep learning is going to cool-sounding things like Image Processing (&ldquo;Is that a bird? Is it a plane? No, it&rsquo;s Superman!&rdquo;), speech recognition (&ldquo;The rain in spain falls mainly in the plain&rdquo;), and their ilk. The commonality between these problems is that the data that the algorithms use are <strong>unstructured</strong>. The reason they are so popular is that our brain also seems to think in an unstructured form (I know mine does!) and so maybe we can relate better to these problems.</p>

<p>However, great economic value has been obtained by applying NNs to <strong>structured</strong> data as well. One of the areas that has received the greatest bump in awesomeness because of Deep Learning, for example, is Ad Pricing. You know how when you search for an product on Google, suddenly Facebook is showing you ads for the product. Well, someone is choosing to bid to show that ad to you at that moment, and they&rsquo;re basing their decision on many computers running many iterations of Neural Networks on all the data they have on you!</p>

<h2 id="deep-learning-and-why-it-s-suddenly-so-popular">Deep Learning and Why It&rsquo;s Suddenly So Popular</h2>

<p>Deep Learning is just the application of Large Neural Networks to problems with large amounts of data.</p>

<p><img src="{{ site.url }}/assets/dl_week1/data_vs_performance.jpg" alt="Performance of Machine Learning Algorithms" />
<em>Performance of Machine Learning Algorithms versus the amount of data available</em></p>

<p>If you stare at the figure above long enough, you&rsquo;ll see what&rsquo;s going on. Deep Learning is in top right of that graph.</p>

<p><mark>EDIT</mark>: It was pointed out to me in the comments below that the previous version of this figure was incorrect. I had drawn it so that neural networks were better than traditional algorithms for <em>all</em> amounts of data. However, that is really not true. When there isn&rsquo;t enough data, it&rsquo;s not clear what algorithm is better. This is the &ldquo;Zone of Mystery&rdquo; marked in the diagram above. In this region, machine learning engineers have to try different algorithms, see which one works best. In this region, the traditional art of machine learning still reigns supreme. What is true is that at some point, traditional algorithms stop improving much. <mark>End Edit</mark></p>

<p>It turns out that the performance of standard machine learning algorithms doesn&rsquo;t improve much if you give them more data. It&rsquo;s almost like they&rsquo;ve reached the limit of their &ldquo;intelligence&rdquo; and giving them more information just doesn&rsquo;t help. So, we turn to neural networks, and keep adding more and more neurons and layers to it and we notice that the performance keeps improving.</p>

<p>Awesome! Let&rsquo;s just use the biggest neural network with the largest amount of data.</p>

<p>Do you see where this is going:</p>

<ol>
<li><strong>Bigger Neural Networks</strong> means more computing power is needed. Such power was not available till the last decade. Now we have supercomputers and all of Google&rsquo;s might, so we have a fighting chance! These days, Google and NVidia and Intel (?) and AMD (?) are all designing chips specifically with Deep Learning in mind.</li>
<li><strong>Large amount of data</strong>: till the advent of the internet, data was siloed and in small amounts. Now with the internet and people writing unnecessary and wordy deep learning tutorials and other people taking needless smartphones photos, there is a deluge of data just waiting to be mined for information.</li>
</ol>

<p>Until the last few years, it was just these two factors that was causing the growth in deep learning performance. But now, we have a new entrant to the arena:</p>

<ol>
<li><strong>Human Creativity!!</strong> Yup, that&rsquo;s right. Deep Learning algorithms won&rsquo;t take your job if you&rsquo;re making the algorithms. It turns out that a simple modification in the algorithms has a <em>huge</em> effect on the performance of Neural Networks. For example, one of the most significant bumps in performance was obtained when the Neural Network funciton was switched from the sigmoid to the RELU.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>As I said in the beginning: <em>Deep Learning is the application of Neural Networks where more than one hidden layer of neurons is involved. In the common form that it has pervaded the media today, it also usually involves a mixture of neural networks with other algorithms specifically applied to large datasets in a variety of areas.</em></p>

<p>Now you know what all of that means. <em>Mic drop</em></p>

    </div>
    <div class="info post_meta">
      <time datetime=0001-01-01T00:00:00Z class="date">Monday, January 1, 0001</time>
      
      
    </div>
    <div class="clearfix"></div>
  </article>
	
		<div class="other_posts">
			
			<a href="https://pritamps.github.io/post/2017-08-12-week-2-logistic-regression-and-neural-networks-1/" class="prev"></a>
			
			
			<a href="https://pritamps.github.io/dl-tutorial-series/" class="next">Deep Learning Tutorials</a>
			
		</div>
		<aside id="comments">
</aside>

	
</section>

        <a id="back_to_top" title="Go To Top" href="#">
  <span>
    <svg viewBox="0 0 24 24">
      <path fill="none" d="M0 0h24v24H0z"></path>
      <path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path>
    </svg>
  </span>
</a>

        <footer id="footer">
  <p>
    <span>&copy; 2020 <a href="https://pritamps.github.io" title="Programming Blog">Programming Blog</a> </span>
    <span>Built with <a rel="nofollow" target="_blank" href="https:/gohugo.io">Hugo</a></span>
    <span>Theme by <a rel="nofollow" target="_blank" href="https://github.com/wayjam/hugo-theme-mixedpaper">WayJam</a></span>
  </p>

  <script src="https://pritamps.github.io/js/main.min.eac174914a2c8d5b3cc7d2f766c41a23844a384c95187d52847b0b2e8c2b77a1.js" integrity="sha256-6sF0kUosjVs8x9L3ZsQaI4RKOEyVGH1ShHsLLowrd6E=" crossorigin="anonymous"></script>
</footer>

    </body>
</html>
