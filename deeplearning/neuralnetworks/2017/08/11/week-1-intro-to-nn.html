<!DOCTYPE html>
<html lang="en">

   <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Introduction to Deep Learning</title>
  <meta name="description" content="Table of Contents">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://pritamps.github.io/deeplearning/neuralnetworks/2017/08/11/week-1-intro-to-nn.html">
  <link rel="alternate" type="application/rss+xml" title="Programming Blog" href="/feed.xml">
  
  <!-- Google Fonts - Inconsolata -->
  <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet">
  

  <!-- Load jQuery -->
  <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
  <!-- Load KaTeX -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.1/katex.min.css" integrity="sha384-BDqcjN11/6D69oC63ObubLHNvQR2fNjin6+AzxA3xalB0swTj17TxVV1tL1Q5Png" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.1/katex.min.js" integrity="sha384-sKYm5us3z9/bRQA+cc3gPzqwI5RVgL8vJQx1lpBudr9IzHOR8fnFUH68dz1GsTQw" crossorigin="anonymous"></script>
</head>



  <body>
  <div class="container">
    <header>
      <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">Programming Blog</a>
  
    
      <nav class="site-nav">
        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About Me</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
        <div>
          <a class="page-link" href="/">Home</a>
        </div>
      </nav>
    
  </div>
</header>

    </header>
  
    <main>
      <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Introduction to Deep Learning</h1>
  </header>

  <div class="post-info">
      
    <span class="post-meta">Aug 11, 2017</span>
    <span class="post-categories">Categories: deeplearning, neuralnetworks</span>
  </div>

  <div class="post-content" itemprop="articleBody">
    <h2 class="no_toc" id="table-of-contents">Table of Contents</h2>
<ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#what-is-a-neuron" id="markdown-toc-what-is-a-neuron">What is a Neuron?</a></li>
  <li><a href="#neural-networks" id="markdown-toc-neural-networks">Neural Networks</a>    <ul>
      <li><a href="#applications-and-types-of-neural-networks" id="markdown-toc-applications-and-types-of-neural-networks">Applications and Types of Neural Networks</a></li>
    </ul>
  </li>
  <li><a href="#deep-learning-and-why-its-suddenly-so-popular" id="markdown-toc-deep-learning-and-why-its-suddenly-so-popular">Deep Learning and Why It‚Äôs Suddenly So Popular</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>Technically, <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> is the application of Neural Networks where more than one hidden layer of neurons is involved. In the common form that it has pervaded the media today, it also usually involves a mixture of neural networks with other algorithms specifically applied to large datasets in a variety of areas.</p>

<p>Don‚Äôt know what that means? You‚Äôre in the right place! The idea of this post is to cover just enough ground (in a very shallow way, pun sort-of intended) so that you can say the above paragraph out loud and understand what it means. So the following questions that I‚Äôm sure are burning your stomach will be answered:</p>

<ol>
  <li>What is a neuron?</li>
  <li>What is a neural network?</li>
  <li>What is deep learning?</li>
  <li>Why has it taken off in the last decade or so?</li>
</ol>

<p>This post corresponds roughly to Week 1 in the <a href="https://www.coursera.org/specializations/deep-learning">Coursera Deep Learning Specialisation</a>.</p>

<p>So, let‚Äôs get started!</p>

<h2 id="what-is-a-neuron">What is a Neuron?</h2>

<p>In the not-Computer-Science world a neuron is an organic thing in your body that is the basic unit of the nervous system. Any information that travels to your brain ‚Äì heartbreak for example ‚Äì goes through neurons. The way I understand it (it is important to note here that I am not a biologist), a neuron turns ‚Äúon‚Äù at some level of electrical voltage. A pulse is carried through the neurons to the brain when they are all ‚Äúon‚Äù. That‚Äôs how information ‚Äì the feeling you get when you resolve a bug in your code, for example ‚Äì gets to your brain.</p>

<p>In the Computer Science world, it‚Äôs the same, really. A neuron takes an input, applies a mathemtical transformation to it, and then gives an output.</p>

<p>‚ÄúWait, isn‚Äôt that the same as a mathematical function?‚Äù, one of you asks.</p>

<p>Nope, nope. A neuron actually does two things to achieve its transformation:</p>

<ol>
  <li>It applies a dot product to the inputs with the weights (the weights are a property of the neuron) and adds a threshold. For example, if the input vectors are of type <script type="math/tex"> \vec{x} \in \mathbb{R}_{3 \times 1}  </script>, the weights will be of the form <script type="math/tex"> \vec{w} \in \mathbb{R}_{3 \times 1} </script>, and the output of this step will be <script type="math/tex"> \vec{x} . \vec{w} + b </script></li>
  <li>Applies an activation function to the result of 1 above, i.e <script type="math/tex"> f(\vec{x}.\vec{w} + b) </script> where <script type="math/tex"> f </script> is a function chosen by us.</li>
</ol>

<p>Here is a carefully constructed illutration that shows the in-depth workings of a single neuron as it is commonly represented in Computer Science.</p>

<p><img src="https://pritamps.github.io/assets/dl_week1/nn_basic.jpg" alt="A not-so-helpful illustration of a neuron" />
<em>This illustration doesn‚Äôt illustrate much other than my inexperience with drawing software</em></p>

<p>‚ÄúOkay fine. The function that the neuron applies can be any linear function, right? What is the class of activation function that you can use?‚Äù, someone pipes in from the back.</p>

<p>That‚Äôs actually a very good question, thank you!</p>

<p>When neural networks were first being designed and used, the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> function was pretty popular. The idea of a sigmoid function is to kinda-sorta simulate the behaviour of a switch (Y = 0 when X is negative and Y = 1 when X is positive), but with the property that is continuous and differentiable everywhere. So if the output of our linear function is positive, you get a value close to 1, and if it‚Äôs negative, you get a value of 0. Can you see how that will be useful if you are doing a classification problem where you either have to say ‚ÄúYes‚Äù (1) or ‚ÄúNo‚Äù (0)?</p>

<p>These days however, the RELU (REctified Linear Unit) is much more popular (shown in the ugly figure below). Apparently, it works much better with optimization algorithms such as Gradient Descent ü§∑‚Äç, even though there is that obvious discontinuity in the derivative. Maybe we‚Äôll figure out how this works some day in the future, you and me.</p>

<p><img src="https://pritamps.github.io/assets/dl_week1/relu.jpg" alt="REctified Linear Unit" />
<em>They could have just called it the RLU instead of RELU. Why the silly acronym?</em></p>

<p>So, to summarise, neuron takes input X, applies the RELU function to it, and generates Y as the output, which can either zero or positive. All clear? Good!</p>

<h2 id="neural-networks">Neural Networks</h2>

<p>I‚Äôm sure many of you saw this coming, but guess what neural networks are? They‚Äôre networks of neurons!</p>

<p>Neurons can be stacked together in a variety of ways, some of which are mind-bogglingly complex, but thankfully we don‚Äôt have to think about that yet. Right now, imagine them stacked in layers, each layer feeding into the one ahead of it. Here‚Äôs a figure for ya if my words aren‚Äôt that well chosen.</p>

<p><img src="https://pritamps.github.io/assets/dl_week1/nn_with_layers.jpg" alt="A feedforward neural network" />
<em>A feedforward neural network. Note that the figure is incomplete. Each neuron can link to ALL neurons in the next layer</em></p>

<p>So here‚Äôs what happens when you have the input vector:</p>

<ol>
  <li>Each neuron in each layer has a set of weights. X is fed into the first layer. Each individual neuron outputs the thresholded dot product, to which RELU is applied.</li>
  <li>The output of all those neurons are fed to the next layer.</li>
  <li>This is repeated till suddenly, you have your output value Y.</li>
</ol>

<p>There‚Äôs some handwaving there, I admit, because this stuff is still simmering in the pot of understanding of my mind. I‚Äôll update this post with more clearly chosen words over the next few weeks.</p>

<p>I know at least some of you are looking at that figure and thinking: How do you know how many layers to use? What do the layers mean? How do we decide the individual functions? Wait, what‚Äôs happening?</p>

<p>So, here‚Äôs what <em>we</em> decide:</p>

<ol>
  <li>The number of layers between the input and output</li>
  <li>The number of neurons in each layer</li>
  <li>The function that each neuron applies. By ‚Äúdecide‚Äù here, I mean that each neuron is an RELU. You don‚Äôt get to choose that for the most part!</li>
</ol>

<p><mark>A cool thing</mark>: The algorithm figures out everything else! Everything else, of course, involves the weights on the individual neuron. The middle layers are sometimes called ‚Äúhidden‚Äù, because all you care about are X and Y, and the algorithm figures out everything in between</p>

<p><mark>Another cool thing</mark>: Does that figure look like your brain? I know it doesn‚Äôt look like mine. It took a genius to make this connection: Walter Pitts. He spent most of his time chasing this idea of modeling the brain based on neural networks and unfortunately is not alive today to see the results of his work. You can read an excellent article about him and his amazing and sad life <a href="http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic">here</a></p>

<h3 id="applications-and-types-of-neural-networks">Applications and Types of Neural Networks</h3>

<p>Neural networks are used everywhere these days: from product recommendations to user-click probabilities, from image recognition to self driving cars. There are different types of neural networks, each of which we will get to at different points during this tutorial series:</p>

<ol>
  <li>Standard Neural Nets: Like the ones shown in that awesomely drawn figure above</li>
  <li>Convolutional Neural Nets: Each layer becomes multi-dimensional. Not sure what that means? To be honest, neither am I. We‚Äôll figure it out in a future post. For now, know that CNNs are all the rage in image processing these days</li>
  <li>Recurrent Neural Networks: wWere we make use of sequential patterns in the data, like in natural language. So this is used in speech recognition, language processing, and those kinds of things</li>
  <li>Custom/Hybrid: Where you have different techniques, you can mix-and-match. Custom NNs are used in complex applications such as self driving cars.</li>
</ol>

<p><mark>Important thing</mark>: These days, all the publicity with deep learning is going to cool-sounding things like Image Processing (‚ÄúIs that a bird? Is it a plane? No, it‚Äôs Superman!‚Äù), speech recognition (‚ÄúThe rain in spain falls mainly in the plain‚Äù), and their ilk. The commonality between these problems is that the data that the algorithms use are <strong>unstructured</strong>. The reason they are so popular is that our brain also seems to think in an unstructured form (I know mine does!) and so maybe we can relate better to these problems.</p>

<p>However, great economic value has been obtained by applying NNs to <strong>structured</strong> data as well. One of the areas that has received the greatest bump in awesomeness because of Deep Learning, for example, is Ad Pricing. You know how when you search for an product on Google, suddenly Facebook is showing you ads for the product. Well, someone is choosing to bid to show that ad to you at that moment, and they‚Äôre basing their decision on many computers running many iterations of Neural Networks on all the data they have on you!</p>

<h2 id="deep-learning-and-why-its-suddenly-so-popular">Deep Learning and Why It‚Äôs Suddenly So Popular</h2>

<p>Deep Learning is just the application of Large Neural Networks to problems with large amounts of data.</p>

<p><img src="https://pritamps.github.io/assets/dl_week1/data_vs_performance.jpg" alt="Performance of Machine Learning Algorithms" />
<em>Performance of Machine Learning Algorithms versus the amount of data available</em></p>

<p>If you stare at the figure above long enough, you‚Äôll see what‚Äôs going on. Deep Learning is in top right of that graph.</p>

<p>It turns out that the performance of standard machine learning algorithms doesn‚Äôt improve much if you give them more data. It‚Äôs almost like they‚Äôve reached the limit of their ‚Äúintelligence‚Äù and giving them more information just doesn‚Äôt help. So, we turn to neural networks, and keep adding more and more neurons and layers to it and we notice that the performance keeps improving.</p>

<p>Awesome! Let‚Äôs just use the biggest neural network with the largest amount of data.</p>

<p>Do you see where this is going:</p>

<ol>
  <li><strong>Bigger Neural Networks</strong> means more computing power is needed. Such power was not available till the last decade. Now we have supercomputers and all of Google‚Äôs might, so we have a fighting chance! These days, Google and NVidia and Intel (?) and AMD (?) are all designing chips specifically with Deep Learning in mind.</li>
  <li><strong>Large amount of data</strong>: till the advent of the internet, data was siloed and in small amounts. Now with the internet and people writing unnecessary and wordy deep learning tutorials and other people taking needless smartphones photos, there is a deluge of data just waiting to be mined for information.</li>
</ol>

<p>Until the last few years, it was just these two factors that was causing the growth in deep learning performance. But now, we have a new entrant to the arena:</p>

<ol>
  <li><strong>Human Creativity!!</strong> Yup, that‚Äôs right. Deep Learning algorithms won‚Äôt take your job if you‚Äôre making the algorithms. It turns out that a simple modification in the algorithms has a <em>huge</em> effect on the performance of Neural Networks. For example, one of the most significant bumps in performance was obtained when the Neural Network funciton was switched from the sigmoid to the RELU.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>As I said in the beginning: <em>Deep Learning is the application of Neural Networks where more than one hidden layer of neurons is involved. In the common form that it has pervaded the media today, it also usually involves a mixture of neural networks with other algorithms specifically applied to large datasets in a variety of areas.</em></p>

<p>Now you know what all of that means. <em>Mic drop</em></p>


  </div>
  
    

  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://pritamps.github.io/deeplearning/neuralnetworks/2017/08/11/week-1-intro-to-nn.html';
      this.page.identifier = 'https://pritamps.github.io/deeplearning/neuralnetworks/2017/08/11/week-1-intro-to-nn.html';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://pritamps.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  


  <script type="text/javascript">
    
$("script[type='math/tex']").replaceWith(
  function(){
    var tex = $(this).text();
    return "<span class=\"inline-equation\">" + 
           katex.renderToString(tex.replace("&","")) +
           "</span>";
});

$("script[type='math/tex; mode=display']").replaceWith(
  function(){
    var tex = $(this).text();

    // Need to do this replace here because Javascript 
    // doesn't treat the `&` properly, and the `&` is needed
    // for alignment
    tex = tex.replace(/&amp;/gi, "\u0026")
    return "<div class=\"equation\" align=\"center\">" + 
           katex.renderToString(tex) +
           "</div> <p></p>";
});

  </script>


</article>

    </main>
  
    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Programming Blog</h2>

    <div class="footer-col-wrapper">
      <div class="contact_info">
        <ul class="contact-list">
          <li>
            
              Pritam Sukumar
            
            </li>
            
            <li><a href="mailto:pritamps@gmail.com">pritamps@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="social_media">
        <ul class="social_media_list">
          
          <li>
            <a href="https://github.com/pritamps"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">pritamps</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/pritamps"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">pritamps</span></a>

          </li>
          
        </ul>
      </div>
    </div>

  </div>

  
</footer>

  </div>
</body>

</html>
